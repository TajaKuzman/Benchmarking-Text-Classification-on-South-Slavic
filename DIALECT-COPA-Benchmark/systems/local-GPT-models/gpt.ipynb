{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "url = open(\"local_models_path.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 5) (500, 5) (500, 7) (500, 5) (500, 7) (500, 5) (500, 7) (500, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load the test datasets\n",
    "test_en = pd.read_json(\"../../datasets/copa-en-test.jsonl\", lines=True)\n",
    "test_sl = pd.read_json(\"../../datasets/copa-sl-test.jsonl\", lines=True)\n",
    "test_hr_ckm = pd.read_json(\"../../datasets/copa-hr-ckm-test.jsonl\", lines=True)\n",
    "test_hr = pd.read_json(\"../../datasets/copa-hr-test.jsonl\", lines=True)\n",
    "test_mk = pd.read_json(\"../../datasets/copa-mk-test.jsonl\", lines=True)\n",
    "test_sl_cer = pd.read_json(\"../../datasets/copa-sl-cer-test.jsonl\", lines=True)\n",
    "test_sr = pd.read_json(\"../../datasets/copa-sr-test.jsonl\", lines=True)\n",
    "test_sr_tor = pd.read_json(\"../../datasets/copa-sr-tor-test.jsonl\", lines=True)\n",
    "\n",
    "print(test_en.shape, test_sl.shape, test_hr.shape, test_hr_ckm.shape, test_mk.shape, test_sl_cer.shape, test_sr.shape, test_sr_tor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"copa-en\",\"copa-sl\", \"copa-hr\", \"copa-hr-ckm\", \"copa-mk\", \"copa-sl-cer\", \"copa-sr\", \"copa-sr-tor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_model(model, prompt, url=url):\n",
    "\n",
    "\tclass ReponseStructure(BaseModel):\n",
    "\t\tanswer: int\n",
    "\n",
    "\tdata = {\n",
    "\t    \"model\": model,\n",
    "\t    \"prompt\": prompt,\n",
    "\t    \"stream\": False,\n",
    "\t    \"temperature\": 0,\n",
    "\t    \"format\": ReponseStructure.model_json_schema()\n",
    "\t}\n",
    "\n",
    "\theaders = {\"Content-Type\": \"application/json\",}\n",
    "\tresponse = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "\treturn response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma3:27b\", \"llama3.3:latest\", \"qwen3:32b\", \"deepseek-r1:14b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdf_path = f\"../../datasets/{df_test_name}-test.jsonl\"\n",
    "\n",
    "\tresponses = []\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tfor line in open(df_path):\n",
    "\t\tentry=json.loads(line)\n",
    "\t\tif df_test_name != \"copa-en\":\n",
    "\t\t\tprompt= 'You will be given a task. The task definition is in English, but the task itself is in another language. Here is the task!\\nGiven the premise \"'+entry['premise']+'\",'\n",
    "\t\t\tif entry['question']=='cause':\n",
    "\t\t\t\tprompt+=' and that we are looking for the cause of this premise,'\n",
    "\t\t\telse:\n",
    "\t\t\t\tprompt+=' and that we are looking for the result of this premise, '\n",
    "\t\t\tprompt+=f\"\"\"which hypothesis is more plausible?\\nHypothesis 1: \"{entry['choice1']}\".\\nHypothesis 2: \"{entry['choice2']}\".\n",
    "\t\t\t\t\t\n",
    "\t\t\t### Output format\n",
    "\t\t\t\tReturn a valid JSON dictionary with the following key: 'answer' and a value should be an integer -- either 1 (if hypothesis 1 is more plausible) or 2 (if hypothesis 2 is more plausible).\n",
    "\t\t\t\"\"\"\n",
    "\t\telif df_test_name == \"copa-en\":\n",
    "\t\t\tprompt= 'You will be given a task. The task definition is in English, as is the task itself. Here is the task!\\nGiven the premise \"'+entry['premise']+'\",'\n",
    "\t\t\tif entry['question']=='cause':\n",
    "\t\t\t\tprompt+=' and that we are looking for the cause of this premise,'\n",
    "\t\t\telse:\n",
    "\t\t\t\tprompt+=' and that we are looking for the result of this premise,'\n",
    "\t\t\tprompt+=f\"\"\"which hypothesis is more plausible?\\nHypothesis 1: \"{entry['choice1']}\".\\nHypothesis 2: \"{entry['choice2']}\".\n",
    "\t\t\t\t\t\n",
    "\t\t\t### Output format\n",
    "\t\t\t\tReturn a valid JSON dictionary with the following key: 'answer' and a value should be an integer -- either 1 (if hypothesis 1 is more plausible) or 2 (if hypothesis 2 is more plausible).\n",
    "\t\t\t\"\"\"\n",
    "\n",
    "\t\tif gpt_model == \"GaMS-27B\":\n",
    "\t\t\tgpt_model_path = \"hf.co/mradermacher/GaMS-27B-Instruct-i1-GGUF:i1-Q4_K_M\"\n",
    "\t\telse:\n",
    "\t\t\tgpt_model_path = gpt_model\n",
    "\n",
    "\t\tinitial_response= run_local_model(gpt_model_path, prompt, url=url)\n",
    "\n",
    "\t\tresponse = initial_response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Convert the string into a dictionary\n",
    "\t\tresponse = json.loads(response)\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\t# The true labels are 0 or 1, so you have to change the answer by substracting 1 from it.\n",
    "\t\t\tpredicted = response[\"answer\"]-1\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tpredicted = initial_response\n",
    "\t\t\tprint(\"error with extracting a label:\")\n",
    "\t\t\tprint(initial_response)\n",
    "\t\t\tresponses.append(initial_response)\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for 500 instances - {elapsed_time_min/500} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"NA (zero-shot)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t},\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check that everything works well by evaluating the models on English\n",
    "for model in models:\n",
    "\tprint(model)\n",
    "\tpredict_gpt(\"copa-en\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"copa-en\",\"copa-sl\", \"copa-hr\", \"copa-hr-ckm\", \"copa-mk\", \"copa-sl-cer\", \"copa-sr\", \"copa-sr-tor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma3:27b\n",
      "Prediction finished. It took 5.792300760746002 min for 500 instances - 0.6950760912895203 s per instance.\n",
      "Classification with gemma3:27b on copa-sl finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 5.281671710809072 min for 500 instances - 0.6338006052970886 s per instance.\n",
      "Classification with llama3.3:latest on copa-sl finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 3.2755377888679504 min for 500 instances - 0.39306453466415403 s per instance.\n",
      "Classification with qwen3:32b on copa-sl finished.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models also on Slovenian\n",
    "for model in [\"gemma3:27b\", \"llama3.3:latest\", \"qwen3:32b\"]:\n",
    "\tprint(model)\n",
    "\tpredict_gpt(\"copa-sl\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma3:27b\n",
      "Prediction finished. It took 5.592840758959452 min for 500 instances - 0.6711408910751343 s per instance.\n",
      "Classification with gemma3:27b on copa-hr finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 19.373022063573202 min for 500 instances - 2.324762647628784 s per instance.\n",
      "Classification with llama3.3:latest on copa-hr finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 3.1452640771865843 min for 500 instances - 0.3774316892623901 s per instance.\n",
      "Classification with qwen3:32b on copa-hr finished.\n",
      "gemma3:27b\n",
      "Prediction finished. It took 5.484848415851593 min for 500 instances - 0.6581818099021912 s per instance.\n",
      "Classification with gemma3:27b on copa-hr-ckm finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 5.325845523675283 min for 500 instances - 0.639101462841034 s per instance.\n",
      "Classification with llama3.3:latest on copa-hr-ckm finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 3.115610110759735 min for 500 instances - 0.37387321329116824 s per instance.\n",
      "Classification with qwen3:32b on copa-hr-ckm finished.\n",
      "gemma3:27b\n",
      "Prediction finished. It took 5.463626825809479 min for 500 instances - 0.6556352190971374 s per instance.\n",
      "Classification with gemma3:27b on copa-mk finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 5.0883976777394615 min for 500 instances - 0.6106077213287353 s per instance.\n",
      "Classification with llama3.3:latest on copa-mk finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 3.1783601085344952 min for 500 instances - 0.38140321302413943 s per instance.\n",
      "Classification with qwen3:32b on copa-mk finished.\n",
      "gemma3:27b\n",
      "Prediction finished. It took 5.547894187768301 min for 500 instances - 0.665747302532196 s per instance.\n",
      "Classification with gemma3:27b on copa-sl-cer finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 5.378577287991842 min for 500 instances - 0.645429274559021 s per instance.\n",
      "Classification with llama3.3:latest on copa-sl-cer finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 3.3030245145161947 min for 500 instances - 0.39636294174194336 s per instance.\n",
      "Classification with qwen3:32b on copa-sl-cer finished.\n",
      "gemma3:27b\n",
      "Prediction finished. It took 5.860711109638214 min for 500 instances - 0.7032853331565857 s per instance.\n",
      "Classification with gemma3:27b on copa-sr finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 5.538690336545309 min for 500 instances - 0.664642840385437 s per instance.\n",
      "Classification with llama3.3:latest on copa-sr finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 3.373759158452352 min for 500 instances - 0.40485109901428223 s per instance.\n",
      "Classification with qwen3:32b on copa-sr finished.\n",
      "gemma3:27b\n",
      "Prediction finished. It took 5.808317716916402 min for 500 instances - 0.6969981260299682 s per instance.\n",
      "Classification with gemma3:27b on copa-sr-tor finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 5.453072826067607 min for 500 instances - 0.6543687391281128 s per instance.\n",
      "Classification with llama3.3:latest on copa-sr-tor finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 3.2683970808982847 min for 500 instances - 0.3922076497077942 s per instance.\n",
      "Classification with qwen3:32b on copa-sr-tor finished.\n"
     ]
    }
   ],
   "source": [
    "# Now, evaluate the models on all other datasets\n",
    "for test in tests[1:]:\n",
    "\tfor model in [\"gemma3:27b\", \"llama3.3:latest\", \"qwen3:32b\"]:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction finished. It took 4.298616341749827 min for 500 instances - 0.5158339610099792 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-en finished.\n",
      "Prediction finished. It took 4.2313922842343645 min for 500 instances - 0.5077670741081238 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-hr finished.\n",
      "Prediction finished. It took 4.1268409808476765 min for 500 instances - 0.49522091770172116 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-hr-ckm finished.\n",
      "Prediction finished. It took 4.145345358053843 min for 500 instances - 0.4974414429664612 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-mk finished.\n",
      "Prediction finished. It took 4.042402517795563 min for 500 instances - 0.48508830213546755 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-sl-cer finished.\n",
      "Prediction finished. It took 4.092441606521606 min for 500 instances - 0.4910929927825928 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-sr finished.\n",
      "Prediction finished. It took 4.209820747375488 min for 500 instances - 0.5051784896850586 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-sr-tor finished.\n"
     ]
    }
   ],
   "source": [
    "# Add the DeepSeek\n",
    "for test in tests:\n",
    "\tpredict_gpt(test, \"deepseek-r1:14b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copa-en',\n",
       " 'copa-sl',\n",
       " 'copa-hr',\n",
       " 'copa-hr-ckm',\n",
       " 'copa-mk',\n",
       " 'copa-sl-cer',\n",
       " 'copa-sr',\n",
       " 'copa-sr-tor']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction finished. It took 4.256381364663442 min for 500 instances - 0.510765763759613 s per instance.\n",
      "Classification with deepseek-r1:14b on copa-sl finished.\n"
     ]
    }
   ],
   "source": [
    "predict_gpt(\"copa-sl\", \"deepseek-r1:14b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'hf.co/mradermacher/GaMS-27B-Instruct-i1-GGUF:i1-Q4_K_M',\n",
       " 'created_at': '2025-10-22T12:37:29.447197783Z',\n",
       " 'response': 'Machine learning is a subset of artificial intelligence that involves training algorithms to learn patterns in data and make predictions or decisions based on new, unseen data.',\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'context': [106,\n",
       "  1645,\n",
       "  108,\n",
       "  74198,\n",
       "  1212,\n",
       "  6479,\n",
       "  6044,\n",
       "  603,\n",
       "  235265,\n",
       "  107,\n",
       "  108,\n",
       "  106,\n",
       "  2516,\n",
       "  108,\n",
       "  24911,\n",
       "  6044,\n",
       "  603,\n",
       "  476,\n",
       "  38397,\n",
       "  576,\n",
       "  18225,\n",
       "  17273,\n",
       "  674,\n",
       "  18348,\n",
       "  4770,\n",
       "  28514,\n",
       "  577,\n",
       "  3918,\n",
       "  12136,\n",
       "  575,\n",
       "  1423,\n",
       "  578,\n",
       "  1501,\n",
       "  32794,\n",
       "  689,\n",
       "  12013,\n",
       "  3482,\n",
       "  611,\n",
       "  888,\n",
       "  235269,\n",
       "  76926,\n",
       "  1423,\n",
       "  235265],\n",
       " 'total_duration': 6886242363,\n",
       " 'load_duration': 6099193403,\n",
       " 'prompt_eval_count': 15,\n",
       " 'prompt_eval_duration': 156567271,\n",
       " 'eval_count': 30,\n",
       " 'eval_duration': 629435991}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://kt-gpu5.ijs.si:11435/api/generate'\n",
    "data = {\n",
    "    \"model\": \"hf.co/mradermacher/GaMS-27B-Instruct-i1-GGUF:i1-Q4_K_M\",\n",
    "    \"prompt\": \"Explain what machine learning is.\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction finished. It took 2.9194748163223267 min for 500 instances - 0.3503369779586792 s per instance.\n",
      "Classification with GaMS-27B on copa-en finished.\n",
      "Prediction finished. It took 2.9133307933807373 min for 500 instances - 0.34959969520568845 s per instance.\n",
      "Classification with GaMS-27B on copa-sl finished.\n",
      "Prediction finished. It took 2.9452760418256125 min for 500 instances - 0.3534331250190735 s per instance.\n",
      "Classification with GaMS-27B on copa-hr finished.\n",
      "Prediction finished. It took 2.948777413368225 min for 500 instances - 0.353853289604187 s per instance.\n",
      "Classification with GaMS-27B on copa-hr-ckm finished.\n",
      "Prediction finished. It took 2.9747700611750285 min for 500 instances - 0.35697240734100344 s per instance.\n",
      "Classification with GaMS-27B on copa-mk finished.\n",
      "Prediction finished. It took 2.9812559445699054 min for 500 instances - 0.35775071334838865 s per instance.\n",
      "Classification with GaMS-27B on copa-sl-cer finished.\n",
      "Prediction finished. It took 2.9419235825538634 min for 500 instances - 0.3530308299064636 s per instance.\n",
      "Classification with GaMS-27B on copa-sr finished.\n",
      "Prediction finished. It took 2.9237649480501813 min for 500 instances - 0.3508517937660217 s per instance.\n",
      "Classification with GaMS-27B on copa-sr-tor finished.\n"
     ]
    }
   ],
   "source": [
    "# Add GaMS:\n",
    "for test in tests:\n",
    "\tpredict_gpt(test, \"GaMS-27B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
