{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "url = open(\"local_models_path.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2600, 14) (2600, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load the test datasets\n",
    "\n",
    "test_en = pd.read_json(\"../../datasets/ParlaSent-EN-test/ParlaSent_EN_test.jsonl\", lines=True)\n",
    "test_bcs = pd.read_json(\"../../datasets/ParlaSent-BSC-test/ParlaSent_BCS_test.jsonl\", lines=True)\n",
    "\n",
    "print(test_en.shape, test_bcs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_model(model, prompt, url=url):\n",
    "\n",
    "\tclass ReponseStructure(BaseModel):\n",
    "\t\tsentiment: int\n",
    "\n",
    "\tdata = {\n",
    "\t    \"model\": model,\n",
    "\t    \"prompt\": prompt,\n",
    "\t    \"stream\": False,\n",
    "\t    \"temperature\": 0,\n",
    "\t    \"format\": ReponseStructure.model_json_schema()\n",
    "\t}\n",
    "\n",
    "\theaders = {\"Content-Type\": \"application/json\",}\n",
    "\tresponse = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "\treturn response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [\"gemma3:27b\", \"deepseek-r1:14b\", \"llama3.3:latest\"]\n",
    "models = [\"llama4:scout\", \"qwen3:32b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdfs = {\n",
    "\t\t\"ParlaSent-EN-test\": test_en,\n",
    "\t\t\"ParlaSent-BCS-test\": test_bcs\n",
    "\t}\n",
    "\n",
    "\tdf = dfs[df_test_name]\n",
    "\n",
    "\tresponses = []\n",
    "\t\n",
    "\ttexts = df[\"text\"].to_list()\n",
    "\tlangs = df[\"lang\"].to_list()\n",
    "\n",
    "\tlabels_dict = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "\tsentiment_description = {\n",
    "\t\t\"Negative - text that is entirely or predominantly negative\":  0, \n",
    "\t\t\"Neutral - text that only contains non-sentiment-related statements\": 1,\n",
    "\t\t\"Positive - text that is entirely or predominantly positive\": 2\n",
    "\t}\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tfor i in list(zip(texts, langs)):\n",
    "\t\ttext = i[0]\n",
    "\t\tlang = i[1]\n",
    "\n",
    "\t\tcurrent_prompt = f\"\"\"\n",
    "\t\t\t### Task\n",
    "\t\t\t\tYour task is to classify the provided parliamentary text into a sentiment label, meaning that you need to recognize whether the speaker's sentiment towards the topic is negative, neutral, positive or somewhere in between. You will be provided with an excerpt from a parliamentary speech in {lang} language, delimited by single quotation marks. Always provide a label, even if you are not sure.\n",
    "\n",
    "\n",
    "\t\t\t### Output format\n",
    "\t\t\t\tReturn a valid JSON dictionary with the following key: 'sentiment' and a value should be an integer which represents one of the labels according to the following dictionary: {sentiment_description}.\n",
    "\n",
    "\t\t\t\tText: '{text}'\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tif gpt_model == \"GaMS-27B-quantized\":\n",
    "\t\t\tgpt_model_path = \"hf.co/mradermacher/GaMS-27B-Instruct-i1-GGUF:i1-Q4_K_M\"\n",
    "\t\telif gpt_model == \"GaMS-27B\":\n",
    "\t\t\tgpt_model_path = \"hf.co/mradermacher/GaMS-27B-Instruct-i1-GGUF:latest\"\n",
    "\t\telse:\n",
    "\t\t\tgpt_model_path = gpt_model\n",
    "\n",
    "\t\tinitial_response= run_local_model(gpt_model_path, current_prompt, url=url)\n",
    "\n",
    "\t\tresponse = initial_response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Convert the string into a dictionary\n",
    "\t\tresponse = json.loads(response)\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\tpredicted = labels_dict[response[\"sentiment\"]]\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tpredicted = initial_response\n",
    "\t\t\tprint(\"error with extracting a label:\")\n",
    "\t\t\tprint(initial_response)\n",
    "\t\t\tresponses.append(\"Mix\")\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for {df.shape[0]} instances - {elapsed_time_min/df.shape[0]} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"NA (zero-shot)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama4:scout\n",
      "Prediction finished. It took 26.681410400072732 min for 2600 instances - 0.6157248553862938 s per instance.\n",
      "Classification with llama4:scout on ParlaSent-EN-test finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 16.51592076619466 min for 2600 instances - 0.38113663306603063 s per instance.\n",
      "Classification with qwen3:32b on ParlaSent-EN-test finished.\n",
      "llama4:scout\n",
      "Prediction finished. It took 28.800346231460573 min for 2600 instances - 0.664623374572167 s per instance.\n",
      "Classification with llama4:scout on ParlaSent-BCS-test finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 17.44969645738602 min for 2600 instances - 0.4026853028627542 s per instance.\n",
      "Classification with qwen3:32b on ParlaSent-BCS-test finished.\n"
     ]
    }
   ],
   "source": [
    "for test in [\"ParlaSent-EN-test\", \"ParlaSent-BCS-test\"]:\n",
    "\tfor model in models:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaMS-27B-quantized\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{ \"sentiment\": 3 }\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "Prediction finished. It took 15.887605230013529 min for 2600 instances - 0.366637043769543 s per instance.\n",
      "Classification with GaMS-27B-quantized on ParlaSent-EN-test finished.\n",
      "GaMS-27B\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "\n",
      "Prediction finished. It took 15.784792617956798 min for 2600 instances - 0.36426444502977223 s per instance.\n",
      "Classification with GaMS-27B on ParlaSent-EN-test finished.\n",
      "GaMS-27B-quantized\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "error with extracting a label:\n",
      "{\"sentiment\": 3}\n",
      "Prediction finished. It took 16.212157424290975 min for 2600 instances - 0.37412670979133017 s per instance.\n",
      "Classification with GaMS-27B-quantized on ParlaSent-BCS-test finished.\n",
      "GaMS-27B\n",
      "Prediction finished. It took 16.22791717449824 min for 2600 instances - 0.37449039633457476 s per instance.\n",
      "Classification with GaMS-27B on ParlaSent-BCS-test finished.\n"
     ]
    }
   ],
   "source": [
    "for test in [\"ParlaSent-EN-test\", \"ParlaSent-BCS-test\"]:\n",
    "\tfor model in [\"GaMS-27B-quantized\", \"GaMS-27B\"]:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
