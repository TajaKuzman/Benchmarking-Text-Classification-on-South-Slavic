{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "client = OpenAI(api_key=open('API_key').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2600, 14) (2600, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load the test datasets\n",
    "\n",
    "test_en = pd.read_json(\"../../datasets/ParlaSent-EN-test/ParlaSent_EN_test.jsonl\", lines=True)\n",
    "test_bcs = pd.read_json(\"../../datasets/ParlaSent-BSC-test/ParlaSent_BCS_test.jsonl\", lines=True)\n",
    "\n",
    "print(test_en.shape, test_bcs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdfs = {\n",
    "\t\t\"ParlaSent-EN-test\": test_en,\n",
    "\t\t\"ParlaSent-BCS-test\": test_bcs\n",
    "\t}\n",
    "\n",
    "\tdf = dfs[df_test_name]\n",
    "\n",
    "\tresponses = []\n",
    "\t\n",
    "\ttexts = df[\"text\"].to_list()\n",
    "\tlangs = df[\"lang\"].to_list()\n",
    "\n",
    "\tlabels_dict = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "\tsentiment_description = {\n",
    "\t\t\"Negative - text that is entirely or predominantly negative\":  0, \n",
    "\t\t\"Neutral - text that only contains non-sentiment-related statements\": 1,\n",
    "\t\t\"Positive - text that is entirely or predominantly positive\": 2\n",
    "\t}\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tfor i in list(zip(texts, langs)):\n",
    "\t\ttext = i[0]\n",
    "\t\tlang = i[1]\n",
    "\t\t# the \"v5\" models do not have the \"temperature\" parameter\n",
    "\t\tif \"gpt-5\" not in gpt_model:\n",
    "\t\t\tcompletion = client.chat.completions.create(model=gpt_model,\n",
    "\t\t\tresponse_format= {\"type\": \"json_object\"},\n",
    "\t\t\tmessages= [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\"content\": f\"\"\"\n",
    "\t\t\t\t### Task\n",
    "\t\t\t\t\tYour task is to classify the provided parliamentary text into a sentiment label, meaning that you need to recognize whether the speaker's sentiment towards the topic is negative, neutral, positive or somewhere in between. You will be provided with an excerpt from a parliamentary speech in {lang} language, delimited by single quotation marks. Always provide a label, even if you are not sure.\n",
    "\n",
    "\n",
    "\t\t\t\t### Output format\n",
    "\t\t\t\t\tReturn a valid JSON dictionary with the following key: 'sentiment' and a value should be an integer which represents one of the labels according to the following dictionary: {sentiment_description}.\n",
    "\n",
    "\t\t\t\t\tText: '{text}'\n",
    "\t\t\t\"\"\"\n",
    "\t\t}\n",
    "\t\t\t],\n",
    "\t\t\ttemperature = 0)\n",
    "\t\t# the \"v5\" models do not have the \"temperature\" parameter\n",
    "\t\telif \"gpt-5\" in gpt_model:\n",
    "\t\t\tcompletion = client.chat.completions.create(model=gpt_model,\n",
    "\t\t\tresponse_format= {\"type\": \"json_object\"},\n",
    "\t\t\tmessages= [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\"content\": f\"\"\"\n",
    "\t\t\t\t### Task\n",
    "\t\t\t\t\tYour task is to classify the provided parliamentary text into a sentiment label, meaning that you need to recognize whether the speaker's sentiment towards the topic is negative, neutral, positive or somewhere in between. You will be provided with an excerpt from a parliamentary speech in {lang} language, delimited by single quotation marks. Always provide a label, even if you are not sure.\n",
    "\n",
    "\n",
    "\t\t\t\t### Output format\n",
    "\t\t\t\t\tReturn a valid JSON dictionary with the following key: 'sentiment' and a value should be an integer which represents one of the labels according to the following dictionary: {sentiment_description}.\n",
    "\n",
    "\t\t\t\t\tText: '{text}'\n",
    "\t\t\t\"\"\"\n",
    "\t\t}\n",
    "\t\t\t],\n",
    "\t\t)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"The model is not supported, check the code.\")\n",
    "\n",
    "\t\tresponse=completion.choices[0].message.content\n",
    "\n",
    "\t\tresponse = response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Convert the string into a dictionary\n",
    "\t\tresponse = json.loads(response)\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\tpredicted = labels_dict[response[\"sentiment\"]]\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tpredicted = \"error\"\n",
    "\t\t\tprint(\"error with extracting a label\")\n",
    "\t\t\tresponses.append(\"Mix\")\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for {df.shape[0]} instances - {elapsed_time_min/df.shape[0]} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"NA (zero-shot)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-2024-08-06\n",
      "Prediction finished. It took 27.62819653749466 min for 2600 instances - 0.6375737662498767 s per instance.\n",
      "Classification with gpt-4o-2024-08-06 on ParlaSent-EN-test finished.\n",
      "gpt-3.5-turbo-0125\n",
      "Prediction finished. It took 27.254899839560192 min for 2600 instances - 0.6289592270667737 s per instance.\n",
      "Classification with gpt-3.5-turbo-0125 on ParlaSent-EN-test finished.\n",
      "gpt-4o-mini-2024-07-18\n",
      "Prediction finished. It took 35.473263736565904 min for 2600 instances - 0.8186137785361364 s per instance.\n",
      "Classification with gpt-4o-mini-2024-07-18 on ParlaSent-EN-test finished.\n",
      "gpt-4o-2024-08-06\n",
      "Prediction finished. It took 28.581721182664236 min for 2600 instances - 0.6595781811384054 s per instance.\n",
      "Classification with gpt-4o-2024-08-06 on ParlaSent-BCS-test finished.\n",
      "gpt-3.5-turbo-0125\n",
      "Prediction finished. It took 28.31333527962367 min for 2600 instances - 0.6533846602990077 s per instance.\n",
      "Classification with gpt-3.5-turbo-0125 on ParlaSent-BCS-test finished.\n",
      "gpt-4o-mini-2024-07-18\n"
     ]
    }
   ],
   "source": [
    "for test in [\"ParlaSent-EN-test\", \"ParlaSent-BCS-test\"]:\n",
    "\tfor model in [\"gpt-4o-2024-08-06\", \"gpt-3.5-turbo-0125\", \"gpt-4o-mini-2024-07-18\"]:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the new v5 models\n",
    "for test in [\"ParlaSent-EN-test\", \"ParlaSent-BCS-test\"]:\n",
    "\tfor model in [\"gpt-5-nano-2025-08-07\", \"gpt-5-mini-2025-08-07\",\"gpt-5\"]:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
