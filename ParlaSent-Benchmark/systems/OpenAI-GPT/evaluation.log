All evaluations completed. The results are added to the `results/results.json` file.
New benchmark scores:

| Model                  | Test Dataset      |   Macro F1 |   Micro F1 |
|:-----------------------|:------------------|-----------:|-----------:|
| gpt-5                  | ParlaSent-EN-test |      0.784 |      0.782 |
| gpt-5-mini-2025-08-07  | ParlaSent-EN-test |      0.773 |      0.771 |
| gpt-4o-2024-08-06      | ParlaSent-EN-test |      0.771 |      0.773 |
| gemma3:27b             | ParlaSent-EN-test |      0.768 |      0.766 |
| gpt-5-nano-2025-08-07  | ParlaSent-EN-test |      0.752 |      0.749 |
| llama3.3:latest        | ParlaSent-EN-test |      0.749 |      0.745 |
| gpt-4o-mini-2024-07-18 | ParlaSent-EN-test |      0.733 |      0.733 |
| XLM-R-ParlaSent        | ParlaSent-EN-test |      0.728 |      0.727 |
| gpt-3.5-turbo-0125     | ParlaSent-EN-test |      0.7   |      0.698 |
| deepseek-r1:14b        | ParlaSent-EN-test |      0.617 |      0.617 |
| SVC                    | ParlaSent-EN-test |      0.346 |      0.395 |
| dummy-stratified       | ParlaSent-EN-test |      0.327 |      0.346 |
| COMPLEMENTNB           | ParlaSent-EN-test |      0.262 |      0.328 |
| dummy-most_frequent    | ParlaSent-EN-test |      0.163 |      0.323 |

------------------------------------------

New benchmark scores:

| Model                  | Test Dataset       |   Macro F1 |   Micro F1 |
|:-----------------------|:-------------------|-----------:|-----------:|
| gpt-5                  | ParlaSent-BCS-test |      0.761 |      0.778 |
| gpt-5-mini-2025-08-07  | ParlaSent-BCS-test |      0.755 |      0.766 |
| gpt-4o-2024-08-06      | ParlaSent-BCS-test |      0.746 |      0.772 |
| gemma3:27b             | ParlaSent-BCS-test |      0.719 |      0.735 |
| gpt-5-nano-2025-08-07  | ParlaSent-BCS-test |      0.713 |      0.721 |
| llama3.3:latest        | ParlaSent-BCS-test |      0.71  |      0.727 |
| XLM-R-ParlaSent        | ParlaSent-BCS-test |      0.703 |      0.725 |
| gpt-4o-mini-2024-07-18 | ParlaSent-BCS-test |      0.679 |      0.708 |
| gpt-3.5-turbo-0125     | ParlaSent-BCS-test |      0.654 |      0.668 |
| deepseek-r1:14b        | ParlaSent-BCS-test |      0.594 |      0.612 |
| COMPLEMENTNB           | ParlaSent-BCS-test |      0.438 |      0.465 |
| SVC                    | ParlaSent-BCS-test |      0.355 |      0.515 |
| dummy-stratified       | ParlaSent-BCS-test |      0.321 |      0.363 |
| dummy-most_frequent    | ParlaSent-BCS-test |      0.204 |      0.441 |

------------------------------------------

| Model                  | Test Dataset      | Language   |   Macro F1 |   Micro F1 |
|:-----------------------|:------------------|:-----------|-----------:|-----------:|
| gpt-5                  | ParlaSent-EN-test | en         |      0.784 |      0.782 |
| gpt-5-mini-2025-08-07  | ParlaSent-EN-test | en         |      0.773 |      0.771 |
| gpt-4o-2024-08-06      | ParlaSent-EN-test | en         |      0.771 |      0.773 |
| gemma3:27b             | ParlaSent-EN-test | en         |      0.768 |      0.766 |
| gpt-5-nano-2025-08-07  | ParlaSent-EN-test | en         |      0.752 |      0.749 |
| llama3.3:latest        | ParlaSent-EN-test | en         |      0.749 |      0.745 |
| gpt-4o-mini-2024-07-18 | ParlaSent-EN-test | en         |      0.733 |      0.733 |
| XLM-R-ParlaSent        | ParlaSent-EN-test | en         |      0.728 |      0.727 |
| gpt-3.5-turbo-0125     | ParlaSent-EN-test | en         |      0.7   |      0.698 |
| deepseek-r1:14b        | ParlaSent-EN-test | en         |      0.617 |      0.617 |
| SVC                    | ParlaSent-EN-test | en         |      0.346 |      0.395 |
| dummy-stratified       | ParlaSent-EN-test | en         |      0.327 |      0.346 |
| COMPLEMENTNB           | ParlaSent-EN-test | en         |      0.262 |      0.328 |
| dummy-most_frequent    | ParlaSent-EN-test | en         |      0.163 |      0.323 |
| Model                  | Test Dataset       | Language   |   Macro F1 |   Micro F1 |
|:-----------------------|:-------------------|:-----------|-----------:|-----------:|
| gpt-4o-2024-08-06      | ParlaSent-BCS-test | hr         |      0.757 |      0.782 |
| gpt-5                  | ParlaSent-BCS-test | hr         |      0.756 |      0.773 |
| gpt-5-mini-2025-08-07  | ParlaSent-BCS-test | hr         |      0.752 |      0.762 |
| gemma3:27b             | ParlaSent-BCS-test | hr         |      0.728 |      0.742 |
| llama3.3:latest        | ParlaSent-BCS-test | hr         |      0.728 |      0.743 |
| gpt-5-nano-2025-08-07  | ParlaSent-BCS-test | hr         |      0.71  |      0.72  |
| XLM-R-ParlaSent        | ParlaSent-BCS-test | hr         |      0.698 |      0.719 |
| gpt-4o-mini-2024-07-18 | ParlaSent-BCS-test | hr         |      0.686 |      0.719 |
| gpt-3.5-turbo-0125     | ParlaSent-BCS-test | hr         |      0.652 |      0.673 |
| deepseek-r1:14b        | ParlaSent-BCS-test | hr         |      0.595 |      0.613 |
| COMPLEMENTNB           | ParlaSent-BCS-test | hr         |      0.432 |      0.452 |
| SVC                    | ParlaSent-BCS-test | hr         |      0.345 |      0.496 |
| dummy-stratified       | ParlaSent-BCS-test | hr         |      0.316 |      0.365 |
| dummy-most_frequent    | ParlaSent-BCS-test | hr         |      0.197 |      0.419 |
| Model                  | Test Dataset       | Language   |   Macro F1 |   Micro F1 |
|:-----------------------|:-------------------|:-----------|-----------:|-----------:|
| gpt-5                  | ParlaSent-BCS-test | bs         |      0.759 |      0.779 |
| gpt-5-mini-2025-08-07  | ParlaSent-BCS-test | bs         |      0.734 |      0.753 |
| gemma3:27b             | ParlaSent-BCS-test | bs         |      0.718 |      0.742 |
| gpt-4o-2024-08-06      | ParlaSent-BCS-test | bs         |      0.701 |      0.747 |
| XLM-R-ParlaSent        | ParlaSent-BCS-test | bs         |      0.697 |      0.721 |
| gpt-5-nano-2025-08-07  | ParlaSent-BCS-test | bs         |      0.684 |      0.695 |
| llama3.3:latest        | ParlaSent-BCS-test | bs         |      0.673 |      0.711 |
| gpt-4o-mini-2024-07-18 | ParlaSent-BCS-test | bs         |      0.593 |      0.653 |
| gpt-3.5-turbo-0125     | ParlaSent-BCS-test | bs         |      0.584 |      0.626 |
| deepseek-r1:14b        | ParlaSent-BCS-test | bs         |      0.568 |      0.589 |
| COMPLEMENTNB           | ParlaSent-BCS-test | bs         |      0.447 |      0.495 |
| SVC                    | ParlaSent-BCS-test | bs         |      0.345 |      0.532 |
| dummy-stratified       | ParlaSent-BCS-test | bs         |      0.32  |      0.363 |
| dummy-most_frequent    | ParlaSent-BCS-test | bs         |      0.216 |      0.479 |
| Model                  | Test Dataset       | Language   |   Macro F1 |   Micro F1 |
|:-----------------------|:-------------------|:-----------|-----------:|-----------:|
| gpt-5                  | ParlaSent-BCS-test | sr         |      0.768 |      0.783 |
| gpt-5-mini-2025-08-07  | ParlaSent-BCS-test | sr         |      0.761 |      0.774 |
| gpt-4o-2024-08-06      | ParlaSent-BCS-test | sr         |      0.738 |      0.764 |
| gpt-5-nano-2025-08-07  | ParlaSent-BCS-test | sr         |      0.72  |      0.727 |
| XLM-R-ParlaSent        | ParlaSent-BCS-test | sr         |      0.71  |      0.732 |
| gemma3:27b             | ParlaSent-BCS-test | sr         |      0.707 |      0.725 |
| llama3.3:latest        | ParlaSent-BCS-test | sr         |      0.693 |      0.711 |
| gpt-4o-mini-2024-07-18 | ParlaSent-BCS-test | sr         |      0.681 |      0.706 |
| gpt-3.5-turbo-0125     | ParlaSent-BCS-test | sr         |      0.664 |      0.67  |
| deepseek-r1:14b        | ParlaSent-BCS-test | sr         |      0.597 |      0.615 |
| COMPLEMENTNB           | ParlaSent-BCS-test | sr         |      0.442 |      0.477 |
| SVC                    | ParlaSent-BCS-test | sr         |      0.37  |      0.535 |
| dummy-stratified       | ParlaSent-BCS-test | sr         |      0.327 |      0.361 |
| dummy-most_frequent    | ParlaSent-BCS-test | sr         |      0.211 |      0.462 |
