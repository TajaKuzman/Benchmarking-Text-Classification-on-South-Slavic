{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=open(\"API_key\", \"r\").read(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 5) (500, 5) (500, 7) (500, 5) (500, 7) (500, 5) (500, 7) (500, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load the test datasets\n",
    "test_en = pd.read_json(\"../../datasets/copa-en-test.jsonl\", lines=True)\n",
    "test_sl = pd.read_json(\"../../datasets/copa-sl-test.jsonl\", lines=True)\n",
    "test_hr_ckm = pd.read_json(\"../../datasets/copa-hr-ckm-test.jsonl\", lines=True)\n",
    "test_hr = pd.read_json(\"../../datasets/copa-hr-test.jsonl\", lines=True)\n",
    "test_mk = pd.read_json(\"../../datasets/copa-mk-test.jsonl\", lines=True)\n",
    "test_sl_cer = pd.read_json(\"../../datasets/copa-sl-cer-test.jsonl\", lines=True)\n",
    "test_sr = pd.read_json(\"../../datasets/copa-sr-test.jsonl\", lines=True)\n",
    "test_sr_tor = pd.read_json(\"../../datasets/copa-sr-tor-test.jsonl\", lines=True)\n",
    "\n",
    "print(test_en.shape, test_sl.shape, test_hr.shape, test_hr_ckm.shape, test_mk.shape, test_sl_cer.shape, test_sr.shape, test_sr_tor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdf_path = f\"../../datasets/{df_test_name}-test.jsonl\"\n",
    "\n",
    "\tresponses = []\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tfor line in open(df_path):\n",
    "\t\tentry=json.loads(line)\n",
    "\n",
    "\t\tif df_test_name != \"copa-en\":\n",
    "\t\t\tprompt= 'You will be given a task. The task definition is in English, but the task itself is in another language. Here is the task!\\nGiven the premise \"'+entry['premise']+'\",'\n",
    "\t\t\tif entry['question']=='cause':\n",
    "\t\t\t\tprompt+=' and that we are looking for the cause of this premise,'\n",
    "\t\t\telse:\n",
    "\t\t\t\tprompt+=' and that we are looking for the result of this premise, '\n",
    "\t\t\tprompt+=f\"\"\"which hypothesis is more plausible?\\nHypothesis 1: \"{entry['choice1']}\".\\nHypothesis 2: \"{entry['choice2']}\".\n",
    "\t\t\t\t\t\n",
    "\t\t\t### Output format\n",
    "\t\t\t\tReturn a valid JSON dictionary with the following key: 'answer' and a value should be an integer -- either 1 (if hypothesis 1 is more plausible) or 2 (if hypothesis 2 is more plausible).\n",
    "\t\t\t\"\"\"\n",
    "\t\telif df_test_name == \"copa-en\":\n",
    "\t\t\tprompt= 'You will be given a task. The task definition is in English, as is the task itself. Here is the task!\\nGiven the premise \"'+entry['premise']+'\",'\n",
    "\t\t\tif entry['question']=='cause':\n",
    "\t\t\t\tprompt+=' and that we are looking for the cause of this premise,'\n",
    "\t\t\telse:\n",
    "\t\t\t\tprompt+=' and that we are looking for the result of this premise,'\n",
    "\t\t\tprompt+=f\"\"\"which hypothesis is more plausible?\\nHypothesis 1: \"{entry['choice1']}\".\\nHypothesis 2: \"{entry['choice2']}\".\n",
    "\t\t\t\t\t\n",
    "\t\t\t### Output format\n",
    "\t\t\t\tReturn a valid JSON dictionary with the following key: 'answer' and a value should be an integer -- either 1 (if hypothesis 1 is more plausible) or 2 (if hypothesis 2 is more plausible).\n",
    "\t\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tif gpt_model != \"anthropic/claude-haiku-4.5\":\n",
    "\t\t\tcompletion = client.chat.completions.create(model=gpt_model,\n",
    "\t\t\t\tresponse_format= {\"type\": \"json_object\"},\n",
    "\t\t\t\tmessages=[\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\t\"content\": prompt}\n",
    "\t\t\t\t],\n",
    "\t\t\t\ttemperature = 0\n",
    "\t\t\t)\n",
    "\t\t# anthropic requires additional rules, as it doesn't support json output formatting\n",
    "\t\telif gpt_model == \"anthropic/claude-haiku-4.5\":\n",
    "\t\t\tprompt+= \"Answer ONLY with the JSON dictionary, do NOT provide your reasoning or the explanation. Provide only the dictionary with the integer 1 or 2 as the answer!\"\n",
    "\t\t\tcompletion = client.chat.completions.create(model=gpt_model,\n",
    "\t\t\t\tresponse_format= {\"type\": \"json_object\"},\n",
    "\t\t\t\tstop=\"}\",\n",
    "\t\t\t\tmessages=[\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\t\"content\": prompt}\n",
    "\t\t\t\t],\n",
    "\t\t\t\ttemperature = 0)\n",
    "\n",
    "\t\tinitial_response=completion.choices[0].message.content\n",
    "\n",
    "\t\tresponse = initial_response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\t# Convert the string into a dictionary\n",
    "\t\t\tresponse = json.loads(response)\n",
    "\t\t\t\n",
    "\t\t\t# The true labels are 0 or 1, so you have to change the answer by substracting 1 from it.\n",
    "\t\t\tpredicted = response[\"answer\"]-1\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tpredicted = initial_response\n",
    "\t\t\tprint(\"error with extracting a label:\")\n",
    "\t\t\tprint(initial_response)\n",
    "\t\t\tresponses.append(initial_response)\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for 500 instances - {elapsed_time_min/500} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"NA (zero-shot)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# The only thing that needs to be changed in the code from OpenAI\n",
    "\tgpt_model_name = gpt_model.split(\"/\")[1]\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model_name, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model_name, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"copa-en\",\"copa-sl\", \"copa-hr\", \"copa-hr-ckm\", \"copa-mk\", \"copa-sl-cer\", \"copa-sr\", \"copa-sr-tor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's evaluate the models that we evaluated in other tasks as well\n",
    "models = [\"google/gemini-2.5-flash\", \"mistralai/mistral-medium-3.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-mk\n",
      "google/gemini-2.5-flash\n",
      "Prediction finished. It took 4.317918360233307 min for 500 instances - 0.5181502032279969 s per instance.\n",
      "Classification with gemini-2.5-flash on copa-mk finished.\n",
      "mistralai/mistral-medium-3.1\n",
      "Prediction finished. It took 6.425474607944489 min for 500 instances - 0.7710569529533386 s per instance.\n",
      "Classification with mistral-medium-3.1 on copa-mk finished.\n",
      "copa-sl-cer\n",
      "google/gemini-2.5-flash\n",
      "Prediction finished. It took 4.583541476726532 min for 500 instances - 0.5500249772071838 s per instance.\n",
      "Classification with gemini-2.5-flash on copa-sl-cer finished.\n",
      "mistralai/mistral-medium-3.1\n",
      "Prediction finished. It took 6.624265456199646 min for 500 instances - 0.7949118547439575 s per instance.\n",
      "Classification with mistral-medium-3.1 on copa-sl-cer finished.\n",
      "copa-sr\n",
      "google/gemini-2.5-flash\n",
      "Prediction finished. It took 4.576912299791972 min for 500 instances - 0.5492294759750366 s per instance.\n",
      "Classification with gemini-2.5-flash on copa-sr finished.\n",
      "mistralai/mistral-medium-3.1\n",
      "Prediction finished. It took 6.020103096961975 min for 500 instances - 0.722412371635437 s per instance.\n",
      "Classification with mistral-medium-3.1 on copa-sr finished.\n",
      "copa-sr-tor\n",
      "google/gemini-2.5-flash\n",
      "Prediction finished. It took 4.237815244992574 min for 500 instances - 0.5085378293991089 s per instance.\n",
      "Classification with gemini-2.5-flash on copa-sr-tor finished.\n",
      "mistralai/mistral-medium-3.1\n",
      "Prediction finished. It took 6.775988630453745 min for 500 instances - 0.8131186356544494 s per instance.\n",
      "Classification with mistral-medium-3.1 on copa-sr-tor finished.\n"
     ]
    }
   ],
   "source": [
    "# Some were already done, continue with others\n",
    "for test in tests[4:]:\n",
    "\tprint(test)\n",
    "\tfor model in models:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate also the Anthropic model\n",
    "\n",
    "for test in tests[1:]:\n",
    "\tprint(test)\n",
    "\tpredict_gpt(test, \"anthropic/claude-haiku-4.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-mk\n"
     ]
    }
   ],
   "source": [
    "# Add the PRO Gemini model\n",
    "\n",
    "for test in [\"copa-mk\", \"copa-sl-cer\", \"copa-sr\", \"copa-sr-tor\"]:\n",
    "\tprint(test)\n",
    "\tpredict_gpt(test, \"google/gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Do evaluation on English and Slovenian\n",
    "\n",
    "for test in [\"copa-sl\", \"copa-en\"]:\n",
    "\tprint(test)\n",
    "\tpredict_gpt(test, \"google/gemini-2.5-pro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
