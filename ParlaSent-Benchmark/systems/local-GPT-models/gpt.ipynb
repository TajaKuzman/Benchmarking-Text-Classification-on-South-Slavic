{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "url = open(\"local_models_path.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2600, 14) (2600, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load the test datasets\n",
    "\n",
    "test_en = pd.read_json(\"../../datasets/ParlaSent-EN-test/ParlaSent_EN_test.jsonl\", lines=True)\n",
    "test_bcs = pd.read_json(\"../../datasets/ParlaSent-BSC-test/ParlaSent_BCS_test.jsonl\", lines=True)\n",
    "\n",
    "print(test_en.shape, test_bcs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_model(model, prompt, url=url):\n",
    "\n",
    "\tclass ReponseStructure(BaseModel):\n",
    "\t\tsentiment: int\n",
    "\n",
    "\tdata = {\n",
    "\t    \"model\": model,\n",
    "\t    \"prompt\": prompt,\n",
    "\t    \"stream\": False,\n",
    "\t    \"temperature\": 0,\n",
    "\t    \"format\": ReponseStructure.model_json_schema()\n",
    "\t}\n",
    "\n",
    "\theaders = {\"Content-Type\": \"application/json\",}\n",
    "\tresponse = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "\treturn response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma3:27b\", \"deepseek-r1:14b\", \"llama3.3:latest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdfs = {\n",
    "\t\t\"ParlaSent-EN-test\": test_en,\n",
    "\t\t\"ParlaSent-BCS-test\": test_bcs\n",
    "\t}\n",
    "\n",
    "\tdf = dfs[df_test_name]\n",
    "\n",
    "\tresponses = []\n",
    "\t\n",
    "\ttexts = df[\"text\"].to_list()\n",
    "\tlangs = df[\"lang\"].to_list()\n",
    "\n",
    "\tlabels_dict = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "\tsentiment_description = {\n",
    "\t\t\"Negative - text that is entirely or predominantly negative\":  0, \n",
    "\t\t\"Neutral - text that only contains non-sentiment-related statements\": 1,\n",
    "\t\t\"Positive - text that is entirely or predominantly positive\": 2\n",
    "\t}\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tfor i in list(zip(texts, langs)):\n",
    "\t\ttext = i[0]\n",
    "\t\tlang = i[1]\n",
    "\n",
    "\t\tcurrent_prompt = f\"\"\"\n",
    "\t\t\t### Task\n",
    "\t\t\t\tYour task is to classify the provided parliamentary text into a sentiment label, meaning that you need to recognize whether the speaker's sentiment towards the topic is negative, neutral, positive or somewhere in between. You will be provided with an excerpt from a parliamentary speech in {lang} language, delimited by single quotation marks. Always provide a label, even if you are not sure.\n",
    "\n",
    "\n",
    "\t\t\t### Output format\n",
    "\t\t\t\tReturn a valid JSON dictionary with the following key: 'sentiment' and a value should be an integer which represents one of the labels according to the following dictionary: {sentiment_description}.\n",
    "\n",
    "\t\t\t\tText: '{text}'\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tinitial_response= run_local_model(gpt_model, current_prompt, url=url)\n",
    "\n",
    "\t\tresponse = initial_response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Convert the string into a dictionary\n",
    "\t\tresponse = json.loads(response)\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\tpredicted = labels_dict[response[\"sentiment\"]]\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tpredicted = initial_response\n",
    "\t\t\tprint(\"error with extracting a label:\")\n",
    "\t\t\tprint(initial_response)\n",
    "\t\t\tresponses.append(\"Mix\")\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for {df.shape[0]} instances - {elapsed_time_min/df.shape[0]} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"NA (zero-shot)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma3:27b\n",
      "Prediction finished. It took 12.55815504391988 min for 2600 instances - 0.28980357793661266 s per instance.\n",
      "Classification with gemma3:27b on ParlaSent-EN-test finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 17.92105767329534 min for 2600 instances - 0.4135628693837386 s per instance.\n",
      "Classification with deepseek-r1:14b on ParlaSent-EN-test finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 21.460712520281472 min for 2600 instances - 0.4952472120064956 s per instance.\n",
      "Classification with llama3.3:latest on ParlaSent-EN-test finished.\n",
      "gemma3:27b\n",
      "Prediction finished. It took 13.968113350868226 min for 2600 instances - 0.32234107732772826 s per instance.\n",
      "Classification with gemma3:27b on ParlaSent-BCS-test finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 17.320038131872813 min for 2600 instances - 0.3996931876586034 s per instance.\n",
      "Classification with deepseek-r1:14b on ParlaSent-BCS-test finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 28.91738468805949 min for 2600 instances - 0.667324262032142 s per instance.\n",
      "Classification with llama3.3:latest on ParlaSent-BCS-test finished.\n"
     ]
    }
   ],
   "source": [
    "for test in [\"ParlaSent-EN-test\", \"ParlaSent-BCS-test\"]:\n",
    "\tfor model in models:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
