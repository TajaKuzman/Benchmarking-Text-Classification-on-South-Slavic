{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "url = open(\"local_models_path.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['piqa-en', 'piqa-mk', 'piqa-bg', 'piqa-sl', 'piqa-sr_cyrl', 'piqa-hr', 'piqa-sr_latn', 'piqa-bs', 'piqa-sl-cer', 'piqa-hr-ckm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_model(model, prompt, url=url):\n",
    "\n",
    "\tclass ReponseStructure(BaseModel):\n",
    "\t\tanswer: int\n",
    "\n",
    "\tdata = {\n",
    "\t    \"model\": model,\n",
    "\t    \"prompt\": prompt,\n",
    "\t    \"stream\": False,\n",
    "\t    \"temperature\": 0,\n",
    "\t    \"format\": ReponseStructure.model_json_schema()\n",
    "\t}\n",
    "\n",
    "\theaders = {\"Content-Type\": \"application/json\",}\n",
    "\tresponse = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "\treturn response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma3:27b\", \"llama3.3:latest\", \"qwen3:32b\", \"deepseek-r1:14b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdf_path = f\"../../datasets/{df_test_name}.jsonl\"\n",
    "\n",
    "\tresponses = []\n",
    "\tinstance_number = 0\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tfor line in open(df_path):\n",
    "\t\tinstance_number += 1\n",
    "\t\tentry=json.loads(line)\n",
    "\n",
    "\t\tprompt= f\"\"\"\n",
    "\t\t### Task\n",
    "\t\t\tGiven the following situation, which option is more likely to be correct?\n",
    "\n",
    "\t\t\tSituation: {entry['prompt']}\n",
    "\n",
    "\t\t\tOption 0: {entry['solution0']}\n",
    "\n",
    "\t\t\tOption 1: {entry['solution1']}\n",
    "\t\t\t\t\n",
    "\t\t### Output format\n",
    "\t\t\tReturn a valid JSON dictionary with the following key: 'answer' and a value should be either 0 (if option 0 is more plausible) or 1 (if option 1 is more plausible).\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tif gpt_model == \"GaMS-27B\":\n",
    "\t\t\tgpt_model_path = \"hf.co/mradermacher/GaMS-27B-Instruct-i1-GGUF:i1-Q4_K_M\"\n",
    "\t\telse:\n",
    "\t\t\tgpt_model_path = gpt_model\n",
    "\n",
    "\t\tinitial_response= run_local_model(gpt_model_path, prompt, url=url)\n",
    "\n",
    "\t\tresponse = initial_response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\t# Convert the string into a dictionary\n",
    "\t\t\tresponse = json.loads(response)\n",
    "\t\t\tpredicted = response[\"answer\"]\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tpredicted = initial_response\n",
    "\t\t\tprint(\"error with extracting a label:\")\n",
    "\t\t\tprint(initial_response)\n",
    "\t\t\tresponses.append(initial_response)\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for {instance_number} instances - {elapsed_time_min/instance_number} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"NA (zero-shot)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t},\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma3:27b\n",
      "Prediction finished. It took 1.2730936606725056 min for 100 instances - 0.7638561964035034 s per instance.\n",
      "Classification with gemma3:27b on piqa-en finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.274828322728475 min for 100 instances - 0.764896993637085 s per instance.\n",
      "Classification with llama3.3:latest on piqa-en finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7048296411832173 min for 100 instances - 0.42289778470993045 s per instance.\n",
      "Classification with qwen3:32b on piqa-en finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 1.061108652750651 min for 100 instances - 0.6366651916503906 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-en finished.\n"
     ]
    }
   ],
   "source": [
    "# First, check that everything works well by evaluating the models on English\n",
    "for model in models:\n",
    "\tprint(model)\n",
    "\tpredict_gpt(\"piqa-en\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['piqa-en', 'piqa-mk', 'piqa-bg', 'piqa-sl', 'piqa-sr_cyrl', 'piqa-hr', 'piqa-sr_latn', 'piqa-bs', 'piqa-sl-cer', 'piqa-hr-ckm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa-mk\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.3054219563802083 min for 100 instances - 0.783253173828125 s per instance.\n",
      "Classification with gemma3:27b on piqa-mk finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.3037960052490234 min for 100 instances - 0.7822776031494141 s per instance.\n",
      "Classification with llama3.3:latest on piqa-mk finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7130160967508952 min for 100 instances - 0.4278096580505371 s per instance.\n",
      "Classification with qwen3:32b on piqa-mk finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 1.06454310019811 min for 100 instances - 0.638725860118866 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-mk finished.\n",
      "piqa-bg\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.2174264828364054 min for 100 instances - 0.7304558897018433 s per instance.\n",
      "Classification with gemma3:27b on piqa-bg finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.0587066173553468 min for 100 instances - 0.635223970413208 s per instance.\n",
      "Classification with llama3.3:latest on piqa-bg finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7953883568445842 min for 100 instances - 0.4772330141067505 s per instance.\n",
      "Classification with qwen3:32b on piqa-bg finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 0.9882534901301067 min for 100 instances - 0.5929520940780639 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-bg finished.\n",
      "piqa-sl\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.1605687737464905 min for 100 instances - 0.6963412642478943 s per instance.\n",
      "Classification with gemma3:27b on piqa-sl finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.3643625537554422 min for 100 instances - 0.8186175322532654 s per instance.\n",
      "Classification with llama3.3:latest on piqa-sl finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7570520639419556 min for 100 instances - 0.4542312383651733 s per instance.\n",
      "Classification with qwen3:32b on piqa-sl finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 1.0447095513343811 min for 100 instances - 0.6268257308006286 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-sl finished.\n",
      "piqa-sr_cyrl\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.1746676564216614 min for 100 instances - 0.7048005938529969 s per instance.\n",
      "Classification with gemma3:27b on piqa-sr_cyrl finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.4208271344502768 min for 100 instances - 0.852496280670166 s per instance.\n",
      "Classification with llama3.3:latest on piqa-sr_cyrl finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.8077017029126485 min for 100 instances - 0.4846210217475891 s per instance.\n",
      "Classification with qwen3:32b on piqa-sr_cyrl finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 1.013515301545461 min for 100 instances - 0.6081091809272766 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-sr_cyrl finished.\n",
      "piqa-hr\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.1616806944211324 min for 100 instances - 0.6970084166526794 s per instance.\n",
      "Classification with gemma3:27b on piqa-hr finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.3330164035161336 min for 100 instances - 0.7998098421096802 s per instance.\n",
      "Classification with llama3.3:latest on piqa-hr finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7434802412986755 min for 100 instances - 0.44608814477920533 s per instance.\n",
      "Classification with qwen3:32b on piqa-hr finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 1.0416065295537313 min for 100 instances - 0.6249639177322388 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-hr finished.\n",
      "piqa-sr_latn\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.167722797393799 min for 100 instances - 0.7006336784362793 s per instance.\n",
      "Classification with gemma3:27b on piqa-sr_latn finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.3870821118354797 min for 100 instances - 0.8322492671012879 s per instance.\n",
      "Classification with llama3.3:latest on piqa-sr_latn finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7864245136578878 min for 100 instances - 0.4718547081947327 s per instance.\n",
      "Classification with qwen3:32b on piqa-sr_latn finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 1.0495717406272889 min for 100 instances - 0.6297430443763733 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-sr_latn finished.\n",
      "piqa-bs\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.177679193019867 min for 100 instances - 0.7066075158119202 s per instance.\n",
      "Classification with gemma3:27b on piqa-bs finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.3573334217071533 min for 100 instances - 0.814400053024292 s per instance.\n",
      "Classification with llama3.3:latest on piqa-bs finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7832628687222799 min for 100 instances - 0.46995772123336793 s per instance.\n",
      "Classification with qwen3:32b on piqa-bs finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 1.0705366770426432 min for 100 instances - 0.6423220062255859 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-bs finished.\n",
      "piqa-sl-cer\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.1332698345184327 min for 100 instances - 0.6799619007110596 s per instance.\n",
      "Classification with gemma3:27b on piqa-sl-cer finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.3140959064165751 min for 100 instances - 0.788457543849945 s per instance.\n",
      "Classification with llama3.3:latest on piqa-sl-cer finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7427741368611653 min for 100 instances - 0.44566448211669923 s per instance.\n",
      "Classification with qwen3:32b on piqa-sl-cer finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 0.9737332900365193 min for 100 instances - 0.5842399740219116 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-sl-cer finished.\n",
      "piqa-hr-ckm\n",
      "gemma3:27b\n",
      "Prediction finished. It took 1.114430296421051 min for 100 instances - 0.6686581778526306 s per instance.\n",
      "Classification with gemma3:27b on piqa-hr-ckm finished.\n",
      "llama3.3:latest\n",
      "Prediction finished. It took 1.287179744243622 min for 100 instances - 0.7723078465461731 s per instance.\n",
      "Classification with llama3.3:latest on piqa-hr-ckm finished.\n",
      "qwen3:32b\n",
      "Prediction finished. It took 0.7266460617383321 min for 100 instances - 0.43598763704299925 s per instance.\n",
      "Classification with qwen3:32b on piqa-hr-ckm finished.\n",
      "deepseek-r1:14b\n",
      "Prediction finished. It took 0.9873273531595866 min for 100 instances - 0.592396411895752 s per instance.\n",
      "Classification with deepseek-r1:14b on piqa-hr-ckm finished.\n"
     ]
    }
   ],
   "source": [
    "# Now, evaluate the models on all other datasets\n",
    "for test in tests[1:]:\n",
    "\tprint(test)\n",
    "\tfor model in models:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
